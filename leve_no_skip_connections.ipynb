{"cells":[{"cell_type":"markdown","metadata":{"id":"nKCRKXkaBWbG"},"source":["# Initial setup\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wxqw5aQBBWbO"},"outputs":[],"source":["global_debug_flag = False\n","is_colab = False\n","should_wandb = False"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":154666,"status":"ok","timestamp":1712173045652,"user":{"displayName":"Andrei Dragan","userId":"07560271742576145276"},"user_tz":-180},"id":"CNzxk3x1BWbO","outputId":"229ec86b-3ebb-434a-9531-275149f4a79a"},"outputs":[],"source":["# Mount the Google Drive and import the necessary files if running on Google Colab\n","if is_colab:\n","    from google.colab import drive\n","    drive.mount(\"/content/drive\")\n","\n","    %mkdir datasets\n","    %mkdir datasets/nyu_data\n","    !unzip \"/content/drive/MyDrive/datasets/nyu_data.zip\" -d \"/content/datasets/nyu_data\"\n","    !unzip \"/content/drive/MyDrive/datasets/nyu_test_data.zip\" -d \"/content/datasets/nyu_test_data\"\n","    !unzip \"/content/drive/MyDrive/datasets/endoslam_test_data.zip\" -d \"/content/datasets/endoslam_test_data\"\n","    !unzip \"/content/drive/MyDrive/datasets/kitti_test_data.zip\" -d \"/content/datasets/kitti_test_data\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10906,"status":"ok","timestamp":1712173056553,"user":{"displayName":"Andrei Dragan","userId":"07560271742576145276"},"user_tz":-180},"id":"IfdnPWUSBWbO","outputId":"7a51e3f6-6bf0-4946-e6fe-366d7e5e8bb7"},"outputs":[],"source":["# Install weights and biases\n","if is_colab:\n","    !pip install wandb -qU"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1zWjJ3KBBWbP"},"outputs":[],"source":["# Import the necessary libraries\n","import numpy as np\n","import random\n","import os\n","import time\n","import math\n","\n","import pathlib\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","\n","from PIL import Image\n","from itertools import permutations"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"UYyfoKQrBWbP","outputId":"ecd0c365-75af-4ca6-f61f-d835483a212b"},"outputs":[],"source":["# Setup weights and biases\n","if should_wandb:\n","    import wandb\n","\n","    wandb.login()\n","\n","    wandb.init(\n","        project=\"leve\",\n","    )"]},{"cell_type":"markdown","metadata":{"id":"T1oUpw98BWbP"},"source":["# The Data Loaders\n"]},{"cell_type":"markdown","metadata":{"id":"vHlaPXRnBWbP"},"source":["## Utilities\n","\n","A set of classes helpful to transform & augment the data\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":257,"status":"ok","timestamp":1712173304391,"user":{"displayName":"Andrei Dragan","userId":"07560271742576145276"},"user_tz":-180},"id":"SbBEaSwWBWbP"},"outputs":[],"source":["class RandomHorizontalFlip(object):\n","    def __init__(self, probability=0.5):\n","        self.probability = probability\n","\n","    def __call__(self, sample):\n","        image, depth = sample[\"image\"], sample[\"depth\"]\n","\n","        if not isinstance(image, Image.Image):\n","            raise TypeError(\"image should be of type PIL Image. Got {}\".format(type(image)))\n","        if not isinstance(depth, Image.Image):\n","            raise TypeError(\"depth should be of type PIL Image. Got {}\".format(type(depth)))\n","\n","        if random.random() < self.probability:\n","            image = image.transpose(Image.FLIP_LEFT_RIGHT)\n","            depth = depth.transpose(Image.FLIP_LEFT_RIGHT)\n","\n","        return {\"image\": image, \"depth\": depth}"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1712173304802,"user":{"displayName":"Andrei Dragan","userId":"07560271742576145276"},"user_tz":-180},"id":"98cfRDp4BWbP"},"outputs":[],"source":["class RandomChannelSwap(object):\n","    def __init__(self, probability):\n","        self.probability = probability\n","        self.channels_permutations = list(permutations([0, 1, 2], 3))\n","\n","    def __call__(self, sample):\n","        image, depth = sample[\"image\"], sample[\"depth\"]\n","\n","        if not isinstance(image, Image.Image):\n","            raise TypeError(\"image should be of type PIL Image. Got {}\".format(type(image)))\n","        if not isinstance(depth, Image.Image):\n","            raise TypeError(\"depth should be of type PIL Image. Got {}\".format(type(depth)))\n","\n","        if random.random() < self.probability:\n","            image = np.asarray(image)\n","            random_permutation = random.choice(self.channels_permutations)\n","            image = Image.fromarray(image[..., random_permutation])\n","\n","        return {\"image\": image, \"depth\": depth}"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1712173304802,"user":{"displayName":"Andrei Dragan","userId":"07560271742576145276"},"user_tz":-180},"id":"1mqAWummBWbQ"},"outputs":[],"source":["class Resize(object):\n","    def __init__(self, resolution):\n","        self.resize = transforms.Resize(resolution)\n","\n","    def __call__(self, sample):\n","        image, depth = sample[\"image\"], sample[\"depth\"]\n","        image = self.resize(image)\n","        depth = self.resize(depth)\n","        return {\"image\": image, \"depth\": depth}"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1712173304802,"user":{"displayName":"Andrei Dragan","userId":"07560271742576145276"},"user_tz":-180},"id":"7KwTJc4fBWbQ"},"outputs":[],"source":["class ToTensor(object):\n","    def __init__(self, is_train=True, max_depth=1000.0):\n","        self.is_train = is_train\n","        self.max_depth = max_depth\n","\n","    def __call__(self, sample):\n","        image, depth = sample[\"image\"], sample[\"depth\"]\n","        to_tensor = transforms.ToTensor()\n","\n","        # Transform the image to a tensor and normalize it within the range [0.0, 1.0]\n","        image = to_tensor(np.array(image).astype(np.float32) / 255.0)\n","        image = torch.clamp(image, 0.0, 1.0)\n","\n","        # Transform the depth map to a tensor. The normalization depends on whether the sample is for training or not\n","        # For training, use the reciprocal of the depth as described by Ibraheem in [https://arxiv.org/abs/1812.11941]\n","        if self.is_train:\n","            # Start by transforming the depth map to a numpy array\n","            depth = np.array(depth).astype(np.float32)\n","\n","            # Remember where the zero values are\n","            zero_mask = depth == 0.0\n","\n","            # Transform the depth map to a tensor\n","            depth = to_tensor(depth)\n","\n","            # Clamp the depth map pixels' values in the range [max_depth / 100.0, max_depth]\n","            # By doing this, there will be no zero valued pixel inside the depth map when applying the division\n","            depth = torch.clamp(depth, self.max_depth / 100.0, self.max_depth)\n","\n","            # Apply the normalization related to the reciprocal of the depth\n","            depth = self.max_depth / depth\n","\n","            # Bring back the initial zero values\n","            depth[:, zero_mask] = 0.0\n","        else:\n","            # For validation and testing no normalization is applied\n","            depth = to_tensor(np.array(depth).astype(np.float32))\n","\n","        return {\"image\": image, \"depth\": depth}"]},{"cell_type":"markdown","metadata":{"id":"NXUVZMgwBWbQ"},"source":["## NYU Dataset Dataloader\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1712173304802,"user":{"displayName":"Andrei Dragan","userId":"07560271742576145276"},"user_tz":-180},"id":"qhZFHV1RBWbQ"},"outputs":[],"source":["def load_nyu_csv(is_debug=False):\n","    nyu2_train_csv_path = \"datasets/nyu_data/data/nyu2_train.csv\"\n","    nyu2_validation_csv_path = \"datasets/nyu_data/data/nyu2_test.csv\"\n","\n","    # Each [nyu2_train.csv] & [nyu2_validation.csv] contains a rows of tuples of type (path_to_the_image, path_to_the_depth_map)\n","    with open(nyu2_train_csv_path, \"r\") as f_train, open(nyu2_validation_csv_path, \"r\") as f_validation:\n","        nyu2_train = [row.split(\",\") for row in f_train.read().split(\"\\n\") if len(row) > 0]\n","        nyu2_validation = [row.split(\",\") for row in f_validation.read().split(\"\\n\") if len(row) > 0]\n","\n","    if is_debug:\n","        # If in debug mode, use a very small subset of the data\n","        nyu2_train = nyu2_train[:16]\n","        nyu2_validation = nyu2_validation[:16]\n","\n","    print(\"Loaded {} train samples and {} validation samples\".format(len(nyu2_train), len(nyu2_validation)))\n","    return nyu2_train, nyu2_validation"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":362,"status":"ok","timestamp":1712173305162,"user":{"displayName":"Andrei Dragan","userId":"07560271742576145276"},"user_tz":-180},"id":"e9ZxZQdyBWbQ"},"outputs":[],"source":["class NYUTrainDataset(Dataset):\n","    def __init__(self, split, transform=None, is_debug=False):\n","        # This is the dataset used in the training pipeline - it contains both training & validation data\n","        self.split = split\n","        self.transform = transform\n","        self.is_debug = is_debug\n","\n","        if self.split == \"train\":\n","            self.nyu2_data, _ = load_nyu_csv(is_debug)\n","        else:\n","            _, self.nyu2_data = load_nyu_csv(is_debug)\n","\n","    def __getitem__(self, idx):\n","        # Retreive the path to the image and the depth map\n","        path_to_the_image = self.nyu2_data[idx][0]\n","        path_to_the_depth_map = self.nyu2_data[idx][1]\n","\n","        # Open the image and the depth map\n","        image = Image.open(\"datasets/nyu_data/\" + path_to_the_image)\n","        depth = Image.open(\"datasets/nyu_data/\" + path_to_the_depth_map)\n","\n","        # Based on the split, transform the depth image in such a way that each pixel value represents the depth in meters\n","        depth = np.array(depth).astype(np.float32)\n","        if self.split == \"train\":\n","            # For training, normalize each pixel value first and then multiply it with 10 to get the depth in meters\n","            depth = depth / 255.0 * 10.0\n","        elif self.split == \"validation\":\n","            # For anything else other than training (validation or testing), divide each pixel value by 1000 to get the depth in meters\n","            depth = depth * 0.001\n","\n","        # After doing all those transformations on the depth map, transform it back to an Image object, since this is what it is expected to be returned\n","        depth = Image.fromarray(depth)\n","\n","        # Build the output\n","        sample = {\"image\": image, \"depth\": depth}\n","        if self.transform:\n","            sample = self.transform(sample)\n","\n","        sample = {\n","            \"image\": sample[\"image\"],\n","            \"depth\": sample[\"depth\"],\n","            \"image_path\": \"datasets/nyu_data/\" + path_to_the_image,\n","            \"depth_path\": \"datasets/nyu_data/\" + path_to_the_depth_map,\n","        }\n","        return sample\n","\n","    def __len__(self):\n","        return len(self.nyu2_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1712173305162,"user":{"displayName":"Andrei Dragan","userId":"07560271742576145276"},"user_tz":-180},"id":"vSu1YP8QBWbQ"},"outputs":[],"source":["class NYUTestDataset(Dataset):\n","    def __init__(self):\n","        self.directory = \"datasets/nyu_test_data/\" if not is_colab else \"datasets/nyu_test_data/nyu_test_data/\"\n","        self.files = os.listdir(self.directory)\n","        self.files.sort()\n","\n","    def __getitem__(self, idx):\n","        sample = np.load(self.directory + self.files[idx])\n","        image, depth = sample[\"image\"], sample[\"depth\"]\n","        image = np.array(image)\n","        depth = np.array(depth)\n","\n","        return image, depth\n","\n","    def __len__(self):\n","        return len(self.files)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1712173305162,"user":{"displayName":"Andrei Dragan","userId":"07560271742576145276"},"user_tz":-180},"id":"g-tKTWUDBWbQ"},"outputs":[],"source":["def train_nyu_transform(resolution):\n","    return transforms.Compose([Resize(resolution), RandomHorizontalFlip(0.5), RandomChannelSwap(0.5), ToTensor(is_train=True, max_depth=10.0)])\n","\n","\n","def validation_nyu_transform(resolution):\n","    return transforms.Compose([Resize(resolution), ToTensor(is_train=False, max_depth=10.0)])"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1712173305162,"user":{"displayName":"Andrei Dragan","userId":"07560271742576145276"},"user_tz":-180},"id":"TVRp4n5IBWbR"},"outputs":[],"source":["def get_nyu_dataset(split, resolution=(480, 640), is_debug=False):\n","    if split == \"test\":\n","        dataset = NYUTestDataset()\n","    elif split == \"train\":\n","        dataset = NYUTrainDataset(split=split, transform=train_nyu_transform(resolution), is_debug=is_debug)\n","    elif split == \"validation\":\n","        dataset = NYUTrainDataset(split=split, transform=validation_nyu_transform(resolution), is_debug=is_debug)\n","\n","    return dataset"]},{"cell_type":"markdown","metadata":{},"source":["## Kitti Dataset Loader\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class KITTIDataset(Dataset):\n","    def __init__(self):\n","        self.directory = \"datasets/kitti_test_data/\" if not is_colab else \"datasets/kitti_test_data/kitti_test_data/\"\n","        self.files = os.listdir(self.directory)\n","        self.files.sort()\n","\n","    def __getitem__(self, idx):\n","        sample = np.load(self.directory + self.files[idx])\n","        image, depth = sample[\"image\"], sample[\"depth\"]\n","        image = np.array(image)\n","        depth = np.array(depth)\n","\n","        return image, depth\n","\n","    def __len__(self):\n","        return len(self.files)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_kitti_dataset():\n","    return KITTIDataset()"]},{"cell_type":"markdown","metadata":{},"source":["## EndoSLAM Dataset Loader\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class EndoSLAMDataset(Dataset):\n","    def __init__(self):\n","        self.directory = \"datasets/endoslam_test_data/\" if not is_colab else \"datasets/endoslam_test_data/endoslam_test_data/\"\n","        self.frame_files = os.listdir(self.directory + \"frames/\")\n","        self.frame_files.sort()\n","        self.depth_files = os.listdir(self.directory + \"depths/\")\n","        self.depth_files.sort()\n","\n","    def __getitem__(self, idx):\n","        frame = Image.open(self.directory + \"frames/\" + self.frame_files[idx])\n","        depth = Image.open(self.directory + \"depths/\" + self.depth_files[idx])\n","\n","        frame = np.array(frame)\n","        depth = np.array(depth) / 255.0 + 1e-6\n","\n","        return frame, depth\n","\n","    def __len__(self):\n","        return len(self.frame_files)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_endoslam_dataset():\n","    return EndoSLAMDataset()"]},{"cell_type":"markdown","metadata":{"id":"s4DkLeq7BWbR"},"source":["## Generic High-Level Data Loader\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1712173305163,"user":{"displayName":"Andrei Dragan","userId":"07560271742576145276"},"user_tz":-180},"id":"mddGp4gBBWbR"},"outputs":[],"source":["def get_dataloader(dataset_name, split=\"train\", resolution=(480, 640), batch_size=8, is_debug=False):\n","    if dataset_name == \"nyu\":\n","        dataset = get_nyu_dataset(split=split, resolution=resolution, is_debug=is_debug)\n","    elif dataset_name == \"kitti\":\n","        dataset = get_kitti_dataset()\n","    elif dataset_name == \"endoslam\":\n","        dataset = get_endoslam_dataset()\n","    else:\n","        raise ValueError(\"Unknown dataset name: {}\".format(dataset_name))\n","\n","    shuffle_flag = False\n","    if split == \"train\" and is_debug == False:\n","        # Shuffle the data only if the dataset is used for training and it is not in debug mode\n","        shuffle_flag = True\n","\n","    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle_flag)\n","    return dataloader"]},{"cell_type":"markdown","metadata":{"id":"qTVw0qVHBWbS"},"source":["# The Model\n"]},{"cell_type":"markdown","metadata":{"id":"swdzo9h1BWbS"},"source":["## Encoder - DDRNet_23_slim\n","\n","Taken from: https://github.com/mic-rud/GuidedDecoding/blob/main/model/DDRNet_23_slim.py\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1712173307029,"user":{"displayName":"Andrei Dragan","userId":"07560271742576145276"},"user_tz":-180},"id":"T7anL1n6BWbS"},"outputs":[],"source":["BatchNorm2d = nn.BatchNorm2d\n","bn_mom = 0.1"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1712173307030,"user":{"displayName":"Andrei Dragan","userId":"07560271742576145276"},"user_tz":-180},"id":"K7ZHuZaCBWbS"},"outputs":[],"source":["def depthwise(in_channels, kernel_size):\n","    padding = (kernel_size - 1) // 2\n","    assert 2 * padding == kernel_size - 1, \"parameters incorrect. kernel={}, padding={}\".format(kernel_size, padding)\n","    return nn.Sequential(\n","        nn.Conv2d(in_channels, in_channels, kernel_size, stride=1, padding=padding, bias=False, groups=in_channels),\n","        nn.BatchNorm2d(in_channels),\n","        nn.ReLU(inplace=True),\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1712173307030,"user":{"displayName":"Andrei Dragan","userId":"07560271742576145276"},"user_tz":-180},"id":"xC6cxIFmBWbS"},"outputs":[],"source":["def pointwise(in_channels, out_channels):\n","    return nn.Sequential(\n","        nn.Conv2d(in_channels, out_channels, 1, 1, 0, bias=False),\n","        nn.BatchNorm2d(out_channels),\n","        nn.ReLU(inplace=True),\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1712173307030,"user":{"displayName":"Andrei Dragan","userId":"07560271742576145276"},"user_tz":-180},"id":"bGZy1qjqBWbS"},"outputs":[],"source":["def conv3x3(in_planes, out_planes, stride=1):\n","    \"\"\"3x3 convolution with padding\"\"\"\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1712173307030,"user":{"displayName":"Andrei Dragan","userId":"07560271742576145276"},"user_tz":-180},"id":"3pBmaNhfBWbS"},"outputs":[],"source":["class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, inplanes, planes, stride=1, downsample=None, no_relu=False):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = conv3x3(inplanes, planes, stride)\n","        self.bn1 = BatchNorm2d(planes, momentum=bn_mom)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.conv2 = conv3x3(planes, planes)\n","        self.bn2 = BatchNorm2d(planes, momentum=bn_mom)\n","        self.downsample = downsample\n","        self.stride = stride\n","        self.no_relu = no_relu\n","\n","    def forward(self, x):\n","        residual = x\n","\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","\n","        if self.downsample is not None:\n","            residual = self.downsample(x)\n","\n","        out += residual\n","\n","        if self.no_relu:\n","            return out\n","        else:\n","            return self.relu(out)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1712173307030,"user":{"displayName":"Andrei Dragan","userId":"07560271742576145276"},"user_tz":-180},"id":"dVhAIvxuBWbT"},"outputs":[],"source":["class Bottleneck(nn.Module):\n","    expansion = 2\n","\n","    def __init__(self, inplanes, planes, stride=1, downsample=None, no_relu=True):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n","        self.bn1 = BatchNorm2d(planes, momentum=bn_mom)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn2 = BatchNorm2d(planes, momentum=bn_mom)\n","        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)\n","        self.bn3 = BatchNorm2d(planes * self.expansion, momentum=bn_mom)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.downsample = downsample\n","        self.stride = stride\n","        self.no_relu = no_relu\n","\n","    def forward(self, x):\n","        residual = x\n","\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","        out = self.relu(out)\n","\n","        out = self.conv3(out)\n","        out = self.bn3(out)\n","\n","        if self.downsample is not None:\n","            residual = self.downsample(x)\n","\n","        out += residual\n","        if self.no_relu:\n","            return out\n","        else:\n","            return self.relu(out)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1712173307031,"user":{"displayName":"Andrei Dragan","userId":"07560271742576145276"},"user_tz":-180},"id":"TAsCHETwBWbT"},"outputs":[],"source":["class DAPPM(nn.Module):\n","    def __init__(self, inplanes, branch_planes, outplanes):\n","        super(DAPPM, self).__init__()\n","        self.scale1 = nn.Sequential(\n","            nn.AvgPool2d(kernel_size=5, stride=2, padding=2),\n","            BatchNorm2d(inplanes, momentum=bn_mom),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n","        )\n","        self.scale2 = nn.Sequential(\n","            nn.AvgPool2d(kernel_size=9, stride=4, padding=4),\n","            BatchNorm2d(inplanes, momentum=bn_mom),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n","        )\n","        self.scale3 = nn.Sequential(\n","            nn.AvgPool2d(kernel_size=17, stride=8, padding=8),\n","            BatchNorm2d(inplanes, momentum=bn_mom),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n","        )\n","        self.scale4 = nn.Sequential(\n","            nn.AdaptiveAvgPool2d((1, 1)),\n","            BatchNorm2d(inplanes, momentum=bn_mom),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n","        )\n","        self.scale0 = nn.Sequential(\n","            BatchNorm2d(inplanes, momentum=bn_mom),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n","        )\n","        self.process1 = nn.Sequential(\n","            BatchNorm2d(branch_planes, momentum=bn_mom),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(branch_planes, branch_planes, kernel_size=3, padding=1, bias=False),\n","        )\n","        self.process2 = nn.Sequential(\n","            BatchNorm2d(branch_planes, momentum=bn_mom),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(branch_planes, branch_planes, kernel_size=3, padding=1, bias=False),\n","        )\n","        self.process3 = nn.Sequential(\n","            BatchNorm2d(branch_planes, momentum=bn_mom),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(branch_planes, branch_planes, kernel_size=3, padding=1, bias=False),\n","        )\n","        self.process4 = nn.Sequential(\n","            BatchNorm2d(branch_planes, momentum=bn_mom),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(branch_planes, branch_planes, kernel_size=3, padding=1, bias=False),\n","        )\n","        self.compression = nn.Sequential(\n","            BatchNorm2d(branch_planes * 5, momentum=bn_mom),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(branch_planes * 5, outplanes, kernel_size=1, bias=False),\n","        )\n","        self.shortcut = nn.Sequential(\n","            BatchNorm2d(inplanes, momentum=bn_mom),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(inplanes, outplanes, kernel_size=1, bias=False),\n","        )\n","\n","    def forward(self, x):\n","        width = x.shape[-1]\n","        height = x.shape[-2]\n","        x_list = []\n","\n","        x_list.append(self.scale0(x))\n","        x_list.append(self.process1((F.interpolate(self.scale1(x), size=[height, width], mode=\"bilinear\") + x_list[0])))\n","        x_list.append((self.process2((F.interpolate(self.scale2(x), size=[height, width], mode=\"bilinear\") + x_list[1]))))\n","        x_list.append(self.process3((F.interpolate(self.scale3(x), size=[height, width], mode=\"bilinear\") + x_list[2])))\n","        x_list.append(self.process4((F.interpolate(self.scale4(x), size=[height, width], mode=\"bilinear\") + x_list[3])))\n","        out = self.compression(torch.cat(x_list, 1)) + self.shortcut(x)\n","        return out"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1712173307031,"user":{"displayName":"Andrei Dragan","userId":"07560271742576145276"},"user_tz":-180},"id":"ligEbEZIBWbT"},"outputs":[],"source":["class Segmenthead(nn.Module):\n","    def __init__(self, inplanes, interplanes, outplanes, scale_factor=None):\n","        super(Segmenthead, self).__init__()\n","        self.bn1 = BatchNorm2d(inplanes, momentum=bn_mom)\n","        self.conv1 = nn.Conv2d(inplanes, interplanes, kernel_size=3, padding=1, bias=False)\n","        self.bn2 = BatchNorm2d(interplanes, momentum=bn_mom)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.conv2 = nn.Conv2d(interplanes, outplanes, kernel_size=1, padding=0, bias=True)\n","        self.scale_factor = scale_factor\n","\n","    def forward(self, x):\n","        x = self.conv1(self.relu(self.bn1(x)))\n","        out = self.conv2(self.relu(self.bn2(x)))\n","\n","        if self.scale_factor is not None:\n","            height = x.shape[-2] * self.scale_factor\n","            width = x.shape[-1] * self.scale_factor\n","            out = F.interpolate(out, size=[height, width], mode=\"bilinear\")\n","\n","        return out"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1712173307031,"user":{"displayName":"Andrei Dragan","userId":"07560271742576145276"},"user_tz":-180},"id":"rYVzBOWOBWbT"},"outputs":[],"source":["class DualResNet(nn.Module):\n","    def __init__(self, block, layers, out_features=19, planes=64, spp_planes=128, head_planes=128, augment=False, skip_out=False):\n","        super(DualResNet, self).__init__()\n","\n","        highres_planes = planes * 2\n","        self.augment = augment\n","        self.skip_out = skip_out\n","\n","        self.conv1 = nn.Sequential(\n","            nn.Conv2d(3, planes, kernel_size=3, stride=2, padding=1),\n","            BatchNorm2d(planes, momentum=bn_mom),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(planes, planes, kernel_size=3, stride=2, padding=1),\n","            BatchNorm2d(planes, momentum=bn_mom),\n","            nn.ReLU(inplace=True),\n","        )\n","\n","        self.relu = nn.ReLU(inplace=False)\n","        self.layer1 = self._make_layer(block, planes, planes, layers[0])\n","        self.layer2 = self._make_layer(block, planes, planes * 2, layers[1], stride=2)\n","        self.layer3 = self._make_layer(block, planes * 2, planes * 4, layers[2], stride=2)\n","        self.layer4 = self._make_layer(block, planes * 4, planes * 8, layers[3], stride=2)\n","\n","        self.compression3 = nn.Sequential(\n","            nn.Conv2d(planes * 4, highres_planes, kernel_size=1, bias=False),\n","            BatchNorm2d(highres_planes, momentum=bn_mom),\n","        )\n","        self.compression4 = nn.Sequential(\n","            nn.Conv2d(planes * 8, highres_planes, kernel_size=1, bias=False),\n","            BatchNorm2d(highres_planes, momentum=bn_mom),\n","        )\n","        self.down3 = nn.Sequential(\n","            nn.Conv2d(highres_planes, planes * 4, kernel_size=3, stride=2, padding=1, bias=False),\n","            BatchNorm2d(planes * 4, momentum=bn_mom),\n","        )\n","        self.down4 = nn.Sequential(\n","            nn.Conv2d(highres_planes, planes * 4, kernel_size=3, stride=2, padding=1, bias=False),\n","            BatchNorm2d(planes * 4, momentum=bn_mom),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(planes * 4, planes * 8, kernel_size=3, stride=2, padding=1, bias=False),\n","            BatchNorm2d(planes * 8, momentum=bn_mom),\n","        )\n","\n","        self.layer3_ = self._make_layer(block, planes * 2, highres_planes, 2)\n","        self.layer4_ = self._make_layer(block, highres_planes, highres_planes, 2)\n","        self.layer5_ = self._make_layer(Bottleneck, highres_planes, highres_planes, 1)\n","        self.layer5 = self._make_layer(Bottleneck, planes * 8, planes * 8, 1, stride=2)\n","        self.spp = DAPPM(planes * 16, spp_planes, planes * 4)\n","        self.final_layer = Segmenthead(planes * 4, head_planes, out_features)\n","\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n","            elif isinstance(m, BatchNorm2d):\n","                nn.init.constant_(m.weight, 1)\n","                nn.init.constant_(m.bias, 0)\n","\n","    def _make_layer(self, block, inplanes, planes, blocks, stride=1):\n","        downsample = None\n","        if stride != 1 or inplanes != planes * block.expansion:\n","            downsample = nn.Sequential(\n","                nn.Conv2d(inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(planes * block.expansion, momentum=bn_mom),\n","            )\n","\n","        layers = []\n","        layers.append(block(inplanes, planes, stride, downsample))\n","        inplanes = planes * block.expansion\n","        for i in range(1, blocks):\n","            if i == (blocks - 1):\n","                layers.append(block(inplanes, planes, stride=1, no_relu=True))\n","            else:\n","                layers.append(block(inplanes, planes, stride=1, no_relu=False))\n","\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        width_output = x.shape[-1] // 8\n","        height_output = x.shape[-2] // 8\n","        layers = []\n","        connections = [x]\n","\n","        x = self.conv1(x)\n","        if self.skip_out:\n","            x1 = x\n","\n","        x = self.layer1(x)\n","        layers.append(x)\n","        connections.append(x)\n","\n","        x = self.layer2(self.relu(x))\n","        layers.append(x)\n","\n","        x = self.layer3(self.relu(x))\n","        layers.append(x)\n","        x_ = self.layer3_(self.relu(layers[1]))\n","\n","        x = x + self.down3(self.relu(x_))\n","        x_ = x_ + F.interpolate(self.compression3(self.relu(layers[2])), size=[height_output, width_output], mode=\"bilinear\")\n","\n","        x = self.layer4(self.relu(x))\n","        layers.append(x)\n","        x_ = self.layer4_(self.relu(x_))\n","\n","        x = x + self.down4(self.relu(x_))\n","        x_ = x_ + F.interpolate(self.compression4(self.relu(layers[3])), size=[height_output, width_output], mode=\"bilinear\")\n","        connections.append(x_)\n","\n","        x_ = self.layer5_(self.relu(x_))\n","        x = F.interpolate(self.spp(self.layer5(self.relu(x))), size=[height_output, width_output], mode=\"bilinear\")\n","\n","        x_ = self.final_layer(x + x_)\n","        return connections, x_"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1712173307031,"user":{"displayName":"Andrei Dragan","userId":"07560271742576145276"},"user_tz":-180},"id":"nuVD3fyZBWbT"},"outputs":[],"source":["def DualResNetBackbone(pretrained=False, features=64):\n","    model = DualResNet(BasicBlock, [2, 2, 2, 2], out_features=features, planes=32, spp_planes=128, head_planes=64, augment=False)\n","    if pretrained:\n","        if is_colab:\n","            checkpoint = torch.load(\"/content/drive/MyDrive/weights/\" + \"DDRNet23s_imagenet.pth\", map_location=\"cpu\")\n","        else:\n","            checkpoint = torch.load(\"weights/\" + \"DDRNet23s_imagenet.pth\", map_location=\"cpu\")\n","\n","        model.load_state_dict(checkpoint, strict=False)\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1712173307031,"user":{"displayName":"Andrei Dragan","userId":"07560271742576145276"},"user_tz":-180},"id":"oTifa_BxBWbU"},"outputs":[],"source":["class Interpolate(nn.Module):\n","    def __init__(self, scale_factor, mode=\"bilinear\"):\n","        super(Interpolate, self).__init__()\n","        self.scale_factor = scale_factor\n","        self.mode = mode\n","\n","    def forward(self, x):\n","        return F.interpolate(x, self.scale_factor, mode=self.mode)"]},{"cell_type":"markdown","metadata":{"id":"OQ4mIoh4BWbU"},"source":["## Leve Encoder\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1712173307031,"user":{"displayName":"Andrei Dragan","userId":"07560271742576145276"},"user_tz":-180},"id":"5cdZMcT7BWbU"},"outputs":[],"source":["class EncoderLayer(nn.Module):\n","    def __init__(self):\n","        super(EncoderLayer, self).__init__()\n","        self.backbone = DualResNetBackbone(pretrained=True, features=64)\n","\n","    def forward(self, x):\n","        return self.backbone(x)"]},{"cell_type":"markdown","metadata":{"id":"AMcUHwbdBWbU"},"source":["## Leve Decoder\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1712173307031,"user":{"displayName":"Andrei Dragan","userId":"07560271742576145276"},"user_tz":-180},"id":"NvkMiVsqBWbU"},"outputs":[],"source":["class LayeredConvolutionBlock(nn.Module):\n","    def __init__(self, num_input_features, num_output_features):\n","        super(LayeredConvolutionBlock, self).__init__()\n","        self.conv1 = nn.Conv2d(num_input_features, num_output_features, kernel_size=3, padding=1)\n","        self.conv2 = nn.Conv2d(num_output_features, num_output_features, kernel_size=3, padding=1)\n","        self.bn1 = nn.BatchNorm2d(num_output_features)\n","        self.bn2 = nn.BatchNorm2d(num_output_features)\n","        self.relu = nn.ReLU(inplace=False)\n","\n","    def forward(self, x):\n","        x = self.relu(self.bn1(self.conv1(x)))\n","        x = self.relu(self.bn2(self.conv2(x)))\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1712173307031,"user":{"displayName":"Andrei Dragan","userId":"07560271742576145276"},"user_tz":-180},"id":"msChLc6mBWbU"},"outputs":[],"source":["class UpsamplingBlock(nn.Module):\n","\n","    def __init__(self, num_input_features, num_output_features, connection_num_features, connection_upscale_factor):\n","        super(UpsamplingBlock, self).__init__()\n","        self.connection_conv = nn.Conv2d(connection_num_features, num_input_features, kernel_size=1, padding=0)\n","        self.dual_conv = LayeredConvolutionBlock(num_input_features, num_input_features)\n","        self.result_conv = nn.Conv2d(num_input_features, num_output_features, kernel_size=1, padding=0)\n","        self.connection_upscale_factor = connection_upscale_factor\n","\n","    def forward(self, x, connection):\n","        upsampled_x = F.interpolate(x, scale_factor=2, mode=\"bilinear\")\n","        x = self.dual_conv(upsampled_x)\n","\n","        if connection is not None:\n","            if self.connection_upscale_factor > 1:\n","                connection = F.interpolate(connection, scale_factor=self.connection_upscale_factor, mode=\"bilinear\")\n","            connection = self.connection_conv(connection)\n","            x = x + connection\n","\n","        x = x + upsampled_x\n","        x = self.result_conv(x)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1712173307032,"user":{"displayName":"Andrei Dragan","userId":"07560271742576145276"},"user_tz":-180},"id":"zrR7ticlBWbU"},"outputs":[],"source":["class DecoderLayer(nn.Module):\n","    def __init__(self, num_features, connection_num_features, connection_upscale_factor):\n","        super(DecoderLayer, self).__init__()\n","        self.upsampling_block1 = UpsamplingBlock(\n","            num_input_features=num_features[0],\n","            num_output_features=num_features[1],\n","            connection_num_features=connection_num_features[0],\n","            connection_upscale_factor=connection_upscale_factor[0],\n","        )\n","        self.upsampling_block2 = UpsamplingBlock(\n","            num_input_features=num_features[1],\n","            num_output_features=num_features[2],\n","            connection_num_features=connection_num_features[1],\n","            connection_upscale_factor=connection_upscale_factor[1],\n","        )\n","        self.upsampling_block3 = UpsamplingBlock(\n","            num_input_features=num_features[2],\n","            num_output_features=1,\n","            connection_num_features=connection_num_features[2],\n","            connection_upscale_factor=connection_upscale_factor[2],\n","        )\n","\n","    def forward(self, x, connections):\n","        x = self.upsampling_block1(x, None)\n","        x = self.upsampling_block2(x, None)\n","        x = self.upsampling_block3(x, None)\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"tnLVZR6QBWbV"},"source":["## Leve\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1712173307032,"user":{"displayName":"Andrei Dragan","userId":"07560271742576145276"},"user_tz":-180},"id":"0Dk8zbG7BWbV"},"outputs":[],"source":["class Leve(nn.Module):\n","    def __init__(self):\n","        super(Leve, self).__init__()\n","        self.encoder = EncoderLayer()\n","        self.decoder = DecoderLayer(num_features=[64, 32, 16], connection_num_features=[64, 32, 3], connection_upscale_factor=[1, 1, 1])\n","\n","    def forward(self, x):\n","        connections, x = self.encoder(x)\n","        x = self.decoder(x, connections)\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"yLPLZPG6BWbV"},"source":["## Utilities\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1712173307032,"user":{"displayName":"Andrei Dragan","userId":"07560271742576145276"},"user_tz":-180},"id":"jzrDZDLeBWbV"},"outputs":[],"source":["def get_model(state_path):\n","    model = Leve()\n","    if state_path != \"\":\n","        model_state_dict = torch.load(state_path, map_location=\"cpu\")\n","        model.load_state_dict(model_state_dict)\n","    return model"]},{"cell_type":"markdown","metadata":{"id":"HE6ff5C_BWbV"},"source":["# The Losses\n","\n","Depth Loss by Alhashim et al. [https://arxiv.org/abs/1812.11941]\n","<br>\n","Some PyTorch code parts taken from [https://arxiv.org/abs/2203.04206] - [https://github.com/mic-rud/GuidedDecoding/blob/main/losses.py]\n"]},{"cell_type":"markdown","metadata":{"id":"pEpV88BABWbW"},"source":["## SSIM Custom Loss\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1712173308799,"user":{"displayName":"Andrei Dragan","userId":"07560271742576145276"},"user_tz":-180},"id":"oD0qC_VQBWbW"},"outputs":[],"source":["def gaussian(window_size, sigma):\n","    gauss = torch.Tensor([math.exp(-((x - window_size // 2) ** 2) / float(2 * sigma**2)) for x in range(window_size)])\n","    return gauss / gauss.sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1712173309290,"user":{"displayName":"Andrei Dragan","userId":"07560271742576145276"},"user_tz":-180},"id":"EIK-rHTHBWbW"},"outputs":[],"source":["def create_window(window_size, channel=1):\n","    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n","    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n","    window = _2D_window.expand(channel, 1, window_size, window_size).contiguous()\n","    return window"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1712173309291,"user":{"displayName":"Andrei Dragan","userId":"07560271742576145276"},"user_tz":-180},"id":"pQIf_fnJBWbW"},"outputs":[],"source":["def ssim(img1, img2, val_range, window_size=11, window=None, size_average=True, full=False):\n","    L = val_range\n","\n","    padd = 0\n","    (_, channel, height, width) = img1.size()\n","    if window is None:\n","        real_size = min(window_size, height, width)\n","        window = create_window(real_size, channel=channel).to(img1.device)\n","        padd = window_size // 2\n","\n","    mu1 = F.conv2d(img1, window, padding=padd, groups=channel)\n","    mu2 = F.conv2d(img2, window, padding=padd, groups=channel)\n","\n","    mu1_sq = mu1.pow(2)\n","    mu2_sq = mu2.pow(2)\n","    mu1_mu2 = mu1 * mu2\n","\n","    sigma1_sq = F.conv2d(img1 * img1, window, padding=padd, groups=channel) - mu1_sq\n","    sigma2_sq = F.conv2d(img2 * img2, window, padding=padd, groups=channel) - mu2_sq\n","    sigma12 = F.conv2d(img1 * img2, window, padding=padd, groups=channel) - mu1_mu2\n","\n","    C1 = (0.01 * L) ** 2\n","    C2 = (0.03 * L) ** 2\n","\n","    v1 = 2.0 * sigma12 + C2\n","    v2 = sigma1_sq + sigma2_sq + C2\n","    cs = torch.mean(v1 / v2)\n","\n","    ssim_map = ((2 * mu1_mu2 + C1) * v1) / ((mu1_sq + mu2_sq + C1) * v2)\n","\n","    if size_average:\n","        ret = ssim_map.mean()\n","    else:\n","        ret = ssim_map.mean(1).mean(1).mean(1)\n","\n","    if full:\n","        return ret, cs\n","\n","    return ret"]},{"cell_type":"markdown","metadata":{"id":"a9BJn5gfBWbW"},"source":["## Gradient Custom Loss\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1712173309291,"user":{"displayName":"Andrei Dragan","userId":"07560271742576145276"},"user_tz":-180},"id":"65eGtSKUBWbX"},"outputs":[],"source":["def gradient(x):\n","    \"\"\"\n","    idea from tf.image.image_gradients(image)\n","    https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/ops/image_ops_impl.py#L3441-L3512\n","    \"\"\"\n","    left = x\n","    right = F.pad(x, [0, 1, 0, 0])[:, :, :, 1:]\n","    top = x\n","    bottom = F.pad(x, [0, 0, 0, 1])[:, :, 1:, :]\n","\n","    dx, dy = right - left, bottom - top\n","\n","    # dx will always have zeros in the last column, right-left\n","    # dy will always have zeros in the last row,    bottom-top\n","    dx[:, :, :, -1] = 0\n","    dy[:, :, -1, :] = 0\n","\n","    return dx, dy"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1712173309291,"user":{"displayName":"Andrei Dragan","userId":"07560271742576145276"},"user_tz":-180},"id":"J6YTwCFeBWbY"},"outputs":[],"source":["def gradient_loss(gen_frames, gt_frames, alpha=1):\n","    gen_dx, gen_dy = gradient(gen_frames)\n","    gt_dx, gt_dy = gradient(gt_frames)\n","\n","    grad_diff_x = torch.abs(gt_dx - gen_dx)\n","    grad_diff_y = torch.abs(gt_dy - gen_dy)\n","\n","    grad_comb = grad_diff_x**alpha + grad_diff_y**alpha\n","\n","    return torch.mean(grad_comb)"]},{"cell_type":"markdown","metadata":{"id":"KnxjRvgBBWbZ"},"source":["## The Actual Loss\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1712173309291,"user":{"displayName":"Andrei Dragan","userId":"07560271742576145276"},"user_tz":-180},"id":"KamIGGDsBWbZ"},"outputs":[],"source":["class Loss:\n","    def __init__(self, alpha, beta, gamma, max_depth=10.0):\n","        self.alpha = alpha\n","        self.beta = beta\n","        self.gamma = gamma\n","        self.max_depth = max_depth\n","        self.L1_loss = nn.L1Loss()\n","\n","    def __call__(self, output, target, knowledge=None):\n","        l_depth1 = self.L1_loss(output, target)\n","        l_ssim1 = torch.clamp(1 - ssim(output, target, self.max_depth) * 0.5, 0, 1)\n","        l_grad1 = gradient_loss(output, target)\n","        loss1 = self.alpha * l_depth1 + self.beta * l_ssim1 + self.gamma * l_grad1\n","        return loss1"]},{"cell_type":"markdown","metadata":{"id":"rZfvxV9gBWbZ"},"source":["# Metrics\n","\n","Code from FastDepth\n","Diana Wofk et al, FastDepth: Fast Monocular Depth\n","Estimation on Embedded Devices, International Conference on Robotics and\n","Automation (ICRA), 2019\n","https://github.com/dwofk/fast-depth\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":239,"status":"ok","timestamp":1712173310998,"user":{"displayName":"Andrei Dragan","userId":"07560271742576145276"},"user_tz":-180},"id":"E1ia1NXkBWbZ"},"outputs":[],"source":["def log10(x):\n","    \"\"\"Convert a new tensor with the base-10 logarithm of the elements of x.\"\"\"\n","    return torch.log(x) / math.log(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1712173311506,"user":{"displayName":"Andrei Dragan","userId":"07560271742576145276"},"user_tz":-180},"id":"C2mubkhSBWbZ"},"outputs":[],"source":["class Result(object):\n","    def __init__(self):\n","        self.irmse, self.imae = 0, 0\n","        self.mse, self.rmse, self.mae = 0, 0, 0\n","        self.absrel, self.lg10 = 0, 0\n","        self.delta1, self.delta2, self.delta3 = 0, 0, 0\n","        self.data_time, self.gpu_time = 0, 0\n","        self.rmse_log = 0\n","\n","    def set_to_worst(self):\n","        self.irmse, self.imae = np.inf, np.inf\n","        self.mse, self.rmse, self.mae = np.inf, np.inf, np.inf\n","        self.rmse_log = np.inf\n","        self.absrel, self.lg10 = np.inf, np.inf\n","        self.delta1, self.delta2, self.delta3 = 0, 0, 0\n","        self.data_time, self.gpu_time = 0, 0\n","\n","    def update(self, irmse, imae, mse, rmse, mae, rmse_log, absrel, lg10, delta1, delta2, delta3, gpu_time, data_time):\n","        self.irmse, self.imae = irmse, imae\n","        self.mse, self.rmse, self.mae = mse, rmse, mae\n","        self.rmse_log = rmse_log\n","        self.absrel, self.lg10 = absrel, lg10\n","        self.delta1, self.delta2, self.delta3 = delta1, delta2, delta3\n","        self.data_time, self.gpu_time = data_time, gpu_time\n","\n","    def evaluate(self, output, target):\n","        abs_diff = (output - target).abs()\n","\n","        self.mse = float((torch.pow(abs_diff, 2)).mean())\n","        self.rmse = math.sqrt(self.mse)\n","        self.mae = float(abs_diff.mean())\n","        self.lg10 = float((log10(output) - log10(target)).abs().mean())\n","        self.rmse_log = math.sqrt(torch.pow(log10(output) - log10(target), 2).mean())\n","        self.absrel = float((abs_diff / target).mean())\n","\n","        maxRatio = torch.max(output / target, target / output)\n","        self.delta1 = float((maxRatio < 1.25).float().mean())\n","        self.delta2 = float((maxRatio < 1.25**2).float().mean())\n","        self.delta3 = float((maxRatio < 1.25**3).float().mean())\n","        self.data_time = 0\n","        self.gpu_time = 0\n","\n","        inv_output = 1 / output\n","        inv_target = 1 / target\n","        abs_inv_diff = (inv_output - inv_target).abs()\n","        self.irmse = math.sqrt((torch.pow(abs_inv_diff, 2)).mean())\n","        self.imae = float(abs_inv_diff.mean())"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1712173311506,"user":{"displayName":"Andrei Dragan","userId":"07560271742576145276"},"user_tz":-180},"id":"KYKjyhJtBWba"},"outputs":[],"source":["class AverageMeter(object):\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.count = 0.0\n","\n","        self.sum_irmse, self.sum_imae = 0, 0\n","        self.sum_mse, self.sum_rmse, self.sum_mae = 0, 0, 0\n","        self.sum_rmse_log = 0\n","        self.sum_absrel, self.sum_lg10 = 0, 0\n","        self.sum_delta1, self.sum_delta2, self.sum_delta3 = 0, 0, 0\n","        self.sum_data_time, self.sum_gpu_time = 0, 0\n","\n","    def update(self, result, gpu_time, data_time, n=1):\n","        self.count += n\n","\n","        self.sum_irmse += n * result.irmse\n","        self.sum_imae += n * result.imae\n","        self.sum_mse += n * result.mse\n","        self.sum_rmse += n * result.rmse\n","        self.sum_rmse_log += n * result.rmse_log\n","        self.sum_mae += n * result.mae\n","        self.sum_absrel += n * result.absrel\n","        self.sum_lg10 += n * result.lg10\n","        self.sum_delta1 += n * result.delta1\n","        self.sum_delta2 += n * result.delta2\n","        self.sum_delta3 += n * result.delta3\n","        self.sum_data_time += n * data_time\n","        self.sum_gpu_time += n * gpu_time\n","\n","    def average(self):\n","        avg = Result()\n","        avg.update(\n","            self.sum_irmse / self.count,\n","            self.sum_imae / self.count,\n","            self.sum_mse / self.count,\n","            self.sum_rmse / self.count,\n","            self.sum_mae / self.count,\n","            self.sum_rmse_log / self.count,\n","            self.sum_absrel / self.count,\n","            self.sum_lg10 / self.count,\n","            self.sum_delta1 / self.count,\n","            self.sum_delta2 / self.count,\n","            self.sum_delta3 / self.count,\n","            self.sum_gpu_time / self.count,\n","            self.sum_data_time / self.count,\n","        )\n","        return avg"]},{"cell_type":"markdown","metadata":{"id":"GWE-KBo3BWba"},"source":["# Training\n"]},{"cell_type":"markdown","metadata":{"id":"637p6lqPBWba"},"source":["## Initialization\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":622,"status":"ok","timestamp":1712173317553,"user":{"displayName":"Andrei Dragan","userId":"07560271742576145276"},"user_tz":-180},"id":"CefVag37BWba"},"outputs":[],"source":["# Training session model details\n","pretrained_checkpoint_path = \"\"\n","state_path = \"\"\n","name = \"leve_no_skip_connections\""]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1712173317553,"user":{"displayName":"Andrei Dragan","userId":"07560271742576145276"},"user_tz":-180},"id":"20TzH8tiBWba"},"outputs":[],"source":["# Training sessions dataset details\n","dataset_name = \"nyu\"\n","resolution = (240, 320)\n","max_depth = 10.0"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":740,"status":"ok","timestamp":1712173318290,"user":{"displayName":"Andrei Dragan","userId":"07560271742576145276"},"user_tz":-180},"id":"MSaB0yzEBWba","outputId":"5c9c027f-9d56-4f23-fa56-f34cf3c8746c"},"outputs":[],"source":["# Create directories to save the necessary information for the training session\n","if pretrained_checkpoint_path == \"\":\n","    id = name + \"__\" + time.strftime(\"%Y_%m_%d-%H_%M\", time.localtime())\n","    root = \"/content/drive/MyDrive/models/trained_models/\" if is_colab else \"models/trained_models/\"\n","    model_dir = root + id\n","else:\n","    model_dir = pretrained_checkpoint_path.split(\"/checkpoints\")[0]\n","\n","checkpoints_dir = model_dir + \"/checkpoints\"\n","results_dir = model_dir + \"/results\"\n","\n","pathlib.Path(model_dir).mkdir(parents=True, exist_ok=True)\n","pathlib.Path(checkpoints_dir).mkdir(parents=True, exist_ok=True)\n","pathlib.Path(results_dir).mkdir(parents=True, exist_ok=True)\n","\n","print(\"Output: \" + model_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3199,"status":"ok","timestamp":1712173321487,"user":{"displayName":"Andrei Dragan","userId":"07560271742576145276"},"user_tz":-180},"id":"UVRz8PVpBWbb","outputId":"dc61a67f-cc5c-4fd3-98a9-7f1df5edc859"},"outputs":[],"source":["# Load the model\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","model = get_model(state_path=state_path).to(device)\n","print(\"Model loaded\")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1712173321488,"user":{"displayName":"Andrei Dragan","userId":"07560271742576145276"},"user_tz":-180},"id":"9tw5jhsMBWbb"},"outputs":[],"source":["# Training sessions parameters\n","epoch = 0\n","num_epochs = 20\n","batch_size = 8 if is_colab else 2\n","val_losses = []\n","loss = Loss(alpha=0.1, beta=1, gamma=1, max_depth=max_depth)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":542,"status":"ok","timestamp":1712173322027,"user":{"displayName":"Andrei Dragan","userId":"07560271742576145276"},"user_tz":-180},"id":"hgjGv7o0BWbb","outputId":"a0e7af33-5475-4393-99d6-213afe1ac71f"},"outputs":[],"source":["# Load the dataloaders\n","train_loader = get_dataloader(dataset_name, split=\"train\", resolution=resolution, batch_size=batch_size, is_debug=global_debug_flag)\n","validation_loader = get_dataloader(dataset_name, split=\"validation\", resolution=resolution, batch_size=batch_size, is_debug=global_debug_flag)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1712173322027,"user":{"displayName":"Andrei Dragan","userId":"07560271742576145276"},"user_tz":-180},"id":"1XUVwSFgBWbb"},"outputs":[],"source":["# Load the checkpoint if necessary\n","if pretrained_checkpoint_path != \"\":\n","    checkpoint = torch.load(pretrained_checkpoint_path, map_location=device)\n","    model.load_state_dict(checkpoint[\"model\"])\n","    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n","    scheduler.load_state_dict(checkpoint[\"scheduler\"])\n","    val_losses = checkpoint[\"val_losses\"]\n","    epoch = checkpoint[\"epoch\"]\n","    epoch_val_loss = checkpoint[\"epoch_val_loss\"]\n","    rmse = checkpoint[\"rmse\"]"]},{"cell_type":"markdown","metadata":{"id":"fuKf3lplBWbb"},"source":["## Utilities\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1712173322027,"user":{"displayName":"Andrei Dragan","userId":"07560271742576145276"},"user_tz":-180},"id":"yExE01w0BWbb"},"outputs":[],"source":["# Functions to normalize and denormalize the depth maps using the previously mentioned reciprocal of the depth\n","def depth_norm(depth):\n","    zero_mask = depth == 0\n","    depth = torch.clamp(depth, max_depth / 100, max_depth)\n","    depth = max_depth / depth\n","    depth[zero_mask] = 0.0\n","    return depth\n","\n","\n","def inverse_depth_norm(depth):\n","    zero_mask = depth == 0\n","    depth = max_depth / depth\n","    depth = torch.clamp(depth, max_depth / 100, max_depth)\n","    depth[zero_mask] = 0.0\n","    return depth"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1712173322027,"user":{"displayName":"Andrei Dragan","userId":"07560271742576145276"},"user_tz":-180},"id":"esGeHbAsBWbb"},"outputs":[],"source":["# Function used to visualize results (taken from [https://github.com/mic-rud/GuidedDecoding/blob/main/training.py])\n","def show_images(image, target, output):\n","    import matplotlib.pyplot as plt\n","\n","    image_np = image[0].cpu().permute(1, 2, 0).numpy()\n","    target[0, 0, target[0, 0] == 100.0] = 0.1\n","\n","    _, axs = plt.subplots(1, 3, figsize=(15, 5))\n","    axs[0].imshow(image_np)\n","    axs[0].set_title(\"Image\")\n","    axs[1].imshow(target[0, 0].cpu())\n","    axs[1].set_title(\"Target\")\n","    axs[2].imshow(output[0, 0].detach().cpu())\n","    axs[2].set_title(\"Output\")\n","\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1712173322027,"user":{"displayName":"Andrei Dragan","userId":"07560271742576145276"},"user_tz":-180},"id":"pE2iuH9DBWbb"},"outputs":[],"source":["def save_checkpoint(epoch_val_loss, rmse):\n","    checkpoint_file = os.path.join(checkpoints_dir, \"checkpoint_{}.pth\".format(epoch))\n","    torch.save(\n","        {\n","            \"epoch\": epoch + 1,\n","            \"val_losses\": val_losses,\n","            \"model\": model.state_dict(),\n","            \"optimizer\": optimizer.state_dict(),\n","            \"scheduler\": scheduler.state_dict(),\n","            \"epoch_val_loss\": epoch_val_loss,\n","            \"rmse\": rmse,\n","        },\n","        checkpoint_file,\n","    )\n","\n","    current_time = time.strftime(\"%H:%M\", time.localtime())\n","    print(\"{} - Model saved\".format(current_time))"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1712173322027,"user":{"displayName":"Andrei Dragan","userId":"07560271742576145276"},"user_tz":-180},"id":"og-l40JMBWbc"},"outputs":[],"source":["def save_model():\n","    # Choose the one with the best RMSE\n","    checkpoints = os.listdir(checkpoints_dir)\n","    checkpoints = [int(checkpoint.split(\".\")[0].split(\"_\")[1]) for checkpoint in checkpoints]\n","    best_epoch = 0\n","    best_rmse = np.inf\n","\n","    for checkpoint in checkpoints:\n","        checkpoint_pth = os.path.join(checkpoints_dir, \"checkpoint_{}.pth\".format(checkpoint))\n","        checkpoint = torch.load(checkpoint_pth)\n","        if checkpoint[\"rmse\"] < best_rmse:\n","            best_rmse = checkpoint[\"rmse\"]\n","            best_epoch = checkpoint[\"epoch\"] - 1\n","\n","    best_checkpoint_pth = os.path.join(checkpoints_dir, \"checkpoint_{}.pth\".format(best_epoch))\n","\n","    best_model_pth = os.path.join(results_dir, \"best_model.pth\")\n","    checkpoint = torch.load(best_checkpoint_pth)\n","\n","    torch.save(checkpoint[\"model\"], best_model_pth)\n","    print(\"Model saved.\")"]},{"cell_type":"markdown","metadata":{"id":"rx6YfjuyBWbc"},"source":["## Validation\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1712173322027,"user":{"displayName":"Andrei Dragan","userId":"07560271742576145276"},"user_tz":-180},"id":"Zb4M_A7KBWbc"},"outputs":[],"source":["def validation_loop(is_end_of_epoch=False):\n","    torch.cuda.empty_cache()\n","    model.eval()\n","    accumulated_loss = 0.0\n","    average_meter = AverageMeter()\n","\n","    with torch.no_grad():\n","        for i, data in enumerate(validation_loader):\n","            t0 = time.time()\n","            image, target = data[\"image\"].to(device), data[\"depth\"].to(device)\n","            data_time = time.time() - t0\n","\n","            t0 = time.time()\n","            output = model(image)\n","            # The model is trained to predict inverse depth due to the reciprocal of the depth\n","            # So to get the actual depth map, the inverse of the output is taken\n","            real_output = inverse_depth_norm(output)\n","            gpu_time = time.time() - t0\n","\n","            # In the case of the validation data, the target is not normalized, so it is necessary to normalize it before calculating the loss\n","            loss_value = loss(output, depth_norm(target))\n","            accumulated_loss += loss_value.item()\n","\n","            if i == 0:\n","                show_images(image, target, real_output)\n","\n","            result = Result()\n","            result.evaluate(real_output.data, target.data)\n","            average_meter.update(result, gpu_time, data_time, image.size(0))\n","\n","    avg = average_meter.average()\n","    current_time = time.strftime(\"%H:%M\", time.localtime())\n","    average_loss = accumulated_loss / (len(validation_loader) + 1)\n","    val_losses.append(average_loss)\n","    print(\"{} - Average Validation Loss: {:3.4f}\".format(current_time, average_loss))\n","\n","    print(\n","        \"RMSE={average.rmse:.3f}\\n\"\n","        \"MAE={average.mae:.3f}\\n\"\n","        \"Delta1={average.delta1:.3f}\\n\"\n","        \"Delta2={average.delta2:.3f}\\n\"\n","        \"Delta3={average.delta3:.3f}\\n\"\n","        \"REL={average.absrel:.3f}\\n\"\n","        \"Lg10={average.lg10:.3f}\\n\"\n","        \"t_GPU={time:.3f}\\n\"\n","        \"*********************\\n\".format(average=avg, time=avg.gpu_time)\n","    )\n","\n","    if should_wandb:\n","        wandb.log(\n","            {\n","                \"validation_loss\": average_loss,\n","                \"rmse\": avg.rmse,\n","                \"mae\": avg.mae,\n","                \"delta1\": avg.delta1,\n","                \"delta2\": avg.delta2,\n","                \"delta3\": avg.delta3,\n","                \"rel\": avg.absrel,\n","                \"lg10\": avg.lg10,\n","                \"t_GPU\": gpu_time,\n","            }\n","        )\n","\n","    if is_end_of_epoch:\n","        save_checkpoint(average_loss, avg.rmse)"]},{"cell_type":"markdown","metadata":{"id":"fmeaHFZ0BWbc"},"source":["## Actual Training\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1712173322028,"user":{"displayName":"Andrei Dragan","userId":"07560271742576145276"},"user_tz":-180},"id":"CfRP0BovBWbc"},"outputs":[],"source":["def training_loop():\n","    model.train()\n","    accumulated_loss = 0.0\n","\n","    for i, data in enumerate(train_loader):\n","        image, target = data[\"image\"].to(device), data[\"depth\"].to(device)\n","\n","        optimizer.zero_grad()\n","        output = model(image)\n","        loss_value = loss(output, target)\n","        loss_value.backward()\n","        optimizer.step()\n","        accumulated_loss += loss_value.item()\n","\n","        print(\"Batch: {} - Loss: {:3.4f}\".format(i, loss_value.item()))\n","        if i % 1000 == 0 and i != 0:\n","            validation_loop(is_end_of_epoch=False)\n","\n","    current_time = time.strftime(\"%H:%M\", time.localtime())\n","    average_loss = accumulated_loss / (len(train_loader) + 1)\n","    print(\"{} - Average Training Loss: {:3.4f}\".format(current_time, average_loss))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1mVf1xQeWGbX4MbYu_csRFv7zNH5TuB0T"},"id":"BBZCbXiABWbc","outputId":"aa46ee2f-9b30-46c6-82ee-b3b5c5730bfc"},"outputs":[],"source":["# Train the model\n","torch.cuda.empty_cache()\n","\n","for epoch in range(epoch, num_epochs):\n","    current_time = time.strftime(\"%H:%M\", time.localtime())\n","    print(\"{} - Epoch {}\".format(current_time, epoch))\n","    training_loop()\n","    validation_loop(is_end_of_epoch=True)\n","\n","save_model()\n","if should_wandb:\n","    wandb.finish()"]},{"cell_type":"markdown","metadata":{},"source":["# Evaluation\n"]},{"cell_type":"markdown","metadata":{},"source":["## Generic evaluation class\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class Evaluation:\n","    def __init__(self, dataset_name, resolution_required_by_model, dataset_resolution, max_depth, crop, model_dir):\n","        # Initialization parameters\n","        self.dataset_name = dataset_name\n","        self.resolution_required_by_model = resolution_required_by_model\n","        self.max_depth = max_depth\n","        self.crop = crop\n","        self.dataset_resolution = dataset_resolution\n","\n","        # Get / Create the corresponding directories\n","        root = \"/content/drive/MyDrive/models/trained_models/\" if is_colab else \"models/trained_models/\"\n","        self.model_dir = root + model_dir\n","        self.results_dir = self.model_dir + \"/\" + dataset_name + \"_results\"\n","        pathlib.Path(self.results_dir).mkdir(parents=True, exist_ok=True)\n","\n","        # Load the model\n","        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","        self.load_model()\n","\n","        # Load the dataloader\n","        self.test_loader = get_dataloader(dataset_name, split=\"test\", resolution=dataset_resolution, batch_size=1, is_debug=False)\n","\n","        # Utilities\n","        self.downscale_image = transforms.Resize(resolution_required_by_model)\n","        self.to_tensor = ToTensor(is_train=False, max_depth=max_depth)\n","        self.images_to_visualize = [0, 100, 200, 300, 400, 500, 600]\n","\n","    def load_model(self):\n","        state_path = self.model_dir + \"/results/best_model.pth\"\n","        self.model = get_model(state_path=state_path).to(self.device)\n","        print(\"Model loaded\")\n","\n","    def save_results(self, average):\n","        results_file = os.path.join(self.results_dir, \"results.txt\")\n","        with open(results_file, \"w\") as f:\n","            f.write(\"RMSE,MAE,REL, RMSE_log,Lg10,Delta1,Delta2,Delta3\\n\")\n","            f.write(\n","                \"{average.rmse:.3f}\"\n","                \",{average.mae:.3f}\"\n","                \",{average.absrel:.3f}\"\n","                \",{average.rmse_log:.3f}\"\n","                \",{average.lg10:.3f}\"\n","                \",{average.delta1:.3f}\"\n","                \",{average.delta2:.3f}\"\n","                \",{average.delta3:.3f}\".format(average=average)\n","            )\n","\n","    def depth_norm(self, depth):\n","        zero_mask = depth == 0\n","        depth = torch.clamp(depth, self.max_depth / 100, self.max_depth)\n","        depth = self.max_depth / depth\n","        depth[zero_mask] = 0.0\n","        return depth\n","\n","    def inverse_depth_norm(self, depth):\n","        zero_mask = depth == 0\n","        depth = self.max_depth / depth\n","        depth = torch.clamp(depth, self.max_depth / 100, self.max_depth)\n","        depth[zero_mask] = 0.0\n","        return depth\n","\n","    def save_image_results(self, image, target, output, image_id):\n","        # Function taken from [https://github.com/mic-rud/GuidedDecoding/blob/main/inference.py]\n","        import matplotlib.pyplot as plt\n","\n","        image = image[0].permute(1, 2, 0).cpu()\n","        target = target[0, 0].permute(0, 1).cpu()\n","        output = output[0, 0].permute(0, 1).detach().cpu()\n","\n","        error_map = target - output\n","        vmax_error = self.max_depth / 10.0\n","        vmin_error = 0.0\n","        cmap = \"viridis\"\n","\n","        vmax = torch.max(target[target != 0.0])\n","        vmin = torch.min(target[target != 0.0])\n","\n","        # Save the image\n","        save_file = os.path.join(self.results_dir, \"image_{}.png\".format(image_id))\n","        fig = plt.figure(frameon=False)\n","        ax = plt.Axes(fig, [0.0, 0.0, 1.0, 1.0])\n","        ax.set_axis_off()\n","        fig.add_axes(ax)\n","        ax.imshow(image)\n","        fig.savefig(save_file)\n","        plt.clf()\n","\n","        # Save the errors\n","        save_file = os.path.join(self.results_dir, \"errors_{}.png\".format(image_id))\n","        fig = plt.figure(frameon=False)\n","        ax = plt.Axes(fig, [0.0, 0.0, 1.0, 1.0])\n","        ax.set_axis_off()\n","        fig.add_axes(ax)\n","        errors = ax.imshow(error_map, vmin=vmin_error, vmax=vmax_error, cmap=\"Reds\")\n","        fig.colorbar(errors, ax=ax, shrink=0.8)\n","        fig.savefig(save_file)\n","        plt.clf()\n","\n","        # Save the ground truth\n","        save_file = os.path.join(self.results_dir, \"target_{}.png\".format(image_id))\n","        fig = plt.figure(frameon=False)\n","        ax = plt.Axes(fig, [0.0, 0.0, 1.0, 1.0])\n","        ax.set_axis_off()\n","        fig.add_axes(ax)\n","        ax.imshow(target, vmin=vmin, vmax=vmax, cmap=cmap)\n","        fig.savefig(save_file)\n","        plt.clf()\n","\n","        # Save the output\n","        save_to_dir = os.path.join(self.results_dir, \"output_{}.png\".format(image_id))\n","        fig = plt.figure(frameon=False)\n","        ax = plt.Axes(fig, [0.0, 0.0, 1.0, 1.0])\n","        ax.set_axis_off()\n","        fig.add_axes(ax)\n","        ax.imshow(output, vmin=vmin, vmax=vmax, cmap=cmap)\n","        fig.savefig(save_to_dir)\n","        plt.clf()\n","\n","        # Close the figures\n","        plt.close(\"all\")\n","\n","    def test_inference_speed(self):\n","        torch.cuda.empty_cache()\n","        times = 0.0\n","\n","        warm_up_runs = 10\n","        num_test_runs = 200\n","        for i in range(num_test_runs + warm_up_runs):\n","            if i == warm_up_runs:\n","                times = 0.0\n","\n","            x = torch.randn([2, 3, *self.dataset_resolution]).to(self.device)\n","            torch.cuda.synchronize()\n","            t0 = time.time()\n","            _ = self.model(x)\n","            torch.cuda.synchronize()\n","            times += time.time() - t0\n","\n","        times = times / num_test_runs\n","        fps = 1 / times\n","        print(\"[PyTorch] Runtime: {}s\".format(times))\n","        print(\"[PyTorch] FPS: {}\\n\".format(fps))\n","        print(\"Number of parameters: {}\".format(sum(p.numel() for p in self.model.parameters() if p.requires_grad)))\n","\n","    def evaluate(self):\n","        self.model.eval()\n","        average_meter = AverageMeter()\n","\n","        for i, data in enumerate(self.test_loader):\n","            # Load the data\n","            t0 = time.time()\n","            sample = self.to_tensor({\"image\": data[0][0], \"depth\": data[1][0]})\n","            image, target = sample[\"image\"].to(self.device), sample[\"depth\"].to(self.device)\n","\n","            image = self.downscale_image(image.unsqueeze(0))\n","            image_flip = self.downscale_image(torch.flip(image, [3]))\n","\n","            target = target.unsqueeze(0)\n","            target_flip = torch.flip(target, [3])\n","\n","            data_time = time.time() - t0\n","\n","            # Forward pass\n","            t0 = time.time()\n","\n","            output = self.model(image)\n","            real_output = self.inverse_depth_norm(output)\n","\n","            output_flip = self.model(image_flip)\n","            real_output_flip = self.inverse_depth_norm(output_flip)\n","\n","            gpu_time = time.time() - t0\n","\n","            # Prepare the results\n","            # Only the input image that goes through the model is downscaled\n","            # This implies that a function to upscale the output of the model to the original size is needed\n","            upscale_image = transforms.Resize(target.shape[-2:])\n","            real_output = upscale_image(real_output)\n","            real_output_flip = upscale_image(real_output_flip)\n","\n","            if i in self.images_to_visualize:\n","                self.save_image_results(image, target, real_output, i)\n","\n","            # Apply the crop\n","            target = target[:, :, self.crop[0] : self.crop[1], self.crop[2] : self.crop[3]]\n","            target_flip = target_flip[:, :, self.crop[0] : self.crop[1], self.crop[2] : self.crop[3]]\n","            real_output = real_output[:, :, self.crop[0] : self.crop[1], self.crop[2] : self.crop[3]]\n","            real_output_flip = real_output_flip[:, :, self.crop[0] : self.crop[1], self.crop[2] : self.crop[3]]\n","\n","            # Update the results\n","            result = Result()\n","            result.evaluate(real_output.data, target.data)\n","            average_meter.update(result, gpu_time, data_time, image.size(0))\n","\n","            result_flip = Result()\n","            result_flip.evaluate(real_output_flip.data, target_flip.data)\n","            average_meter.update(result_flip, gpu_time, data_time, image.size(0))\n","\n","        # Report the results\n","        avg = average_meter.average()\n","        self.save_results(avg)\n","        print(\n","            \"RMSE={average.rmse:.3f}\\n\"\n","            \"MAE={average.mae:.3f}\\n\"\n","            \"Delta1={average.delta1:.3f}\\n\"\n","            \"Delta2={average.delta2:.3f}\\n\"\n","            \"Delta3={average.delta3:.3f}\\n\"\n","            \"REL={average.absrel:.3f}\\n\"\n","            \"Lg10={average.lg10:.3f}\\n\"\n","            \"t_GPU={time:.3f}\\n\"\n","            \"*********************\\n\".format(average=avg, time=avg.gpu_time)\n","        )"]},{"cell_type":"markdown","metadata":{},"source":["## Parameters\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["my_resolution_required_by_model = (240, 320)\n","my_model_dir = \"leve_no_skip_connections\""]},{"cell_type":"markdown","metadata":{},"source":["## NYU Depth V2\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["nyu_evaluator = Evaluation(\n","    dataset_name=\"nyu\",\n","    resolution_required_by_model=my_resolution_required_by_model,\n","    dataset_resolution=(480, 640),\n","    max_depth=10.0,\n","    crop=[20, 460, 24, 616],\n","    model_dir=my_model_dir,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["nyu_evaluator.evaluate()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["nyu_evaluator.test_inference_speed()"]},{"cell_type":"markdown","metadata":{},"source":["## Kitti\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["kitti_evaluator = Evaluation(\n","    dataset_name=\"kitti\",\n","    resolution_required_by_model=my_resolution_required_by_model,\n","    dataset_resolution=(384, 1280),\n","    max_depth=80.0,\n","    crop=[128, 381, 45, 1196],\n","    model_dir=my_model_dir,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["kitti_evaluator.evaluate()"]},{"cell_type":"markdown","metadata":{},"source":["## EndoSLAM\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["endoslam_evaluator = Evaluation(\n","    dataset_name=\"endoslam\",\n","    resolution_required_by_model=my_resolution_required_by_model,\n","    dataset_resolution=(320, 320),\n","    max_depth=1,\n","    crop=[0, 320, 0, 320],\n","    model_dir=my_model_dir,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["endoslam_evaluator.evaluate()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"V100","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0"}},"nbformat":4,"nbformat_minor":0}
